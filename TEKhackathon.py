# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gf4wb-pvu_zv-a9Bae2PEeV3p32I8geT

Section A: Data Preprocessing
Section B: Data Evaluation
Section C: Model Selection
Section D: Model Evaluation
Section E: Model Improvement
Section F: Future Predictions
Section G: Model Deployment

#Import libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns',None)

"""#Get the data"""

df = pd.read_csv('Telco-Customer-Churn.csv')
df.head()

df.shape

"""# Drop irrevalent features"""

df = df.drop('customerID',axis=1)
df= df.drop('TotalCharges',axis=1)

df.info()

df.describe()

for i in df.columns:
    print(df[i].value_counts().to_frame())



df.head()

"""#Detection of missing values"""

df.isna().sum().to_frame('missing values')

"""#Data Visualization


"""

## extraction of numerical columns
num = df.select_dtypes(include=np.number)
num.head()

## extraction of catagorical columns
cat = df.select_dtypes(exclude=np.number)
cat.head()

"""Univariate analysis(distplot,histogram,boxplot,countplot)"""

#distplot
for i in num.columns:
    sns.distplot(num[i])
    print(plt.show())

#Histogram
num.hist(bins=100)
plt.show()

"""#Detection of Outlier"""

plt.figure(figsize=(12,6))
num.boxplot()
plt.show()

"""#countplot"""

sns.countplot(df['SeniorCitizen'])

# build a bar chart for internet services column
df['InternetService'].value_counts().plot(kind='bar')
plt.title('distribution of internet services')
plt.show()

"""# Applying ML models"""

df.columns

df['Contract'] = df['Contract'].replace({'Month-to-month':0,'One year':1,'Two year':2})

df.head()

num= df.select_dtypes(include=np.number)
cat = df.select_dtypes(exclude=np.number)

num.shape

cat.shape

cat = pd.get_dummies(cat,drop_first=True)

cat.head()

df1 = pd.concat([num,cat],axis=1)

df1.head()

X = df1.drop(['Churn_Yes'],axis=1)
y = df1['Churn_Yes']

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(X,y)
print(model.feature_importances_)
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

X = X.loc[:,['Contract','gender_Male','PaymentMethod_Electronic check',
             'InternetService_Fiber optic',
         'MonthlyCharges','tenure']]

X.head()

y.head()

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.35,random_state = 0)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
sc.fit(X_train)

X_train = sc.transform(X_train)
X_test = sc.transform(X_test)
print('X_train:',X_train.shape)
print('X_test:',X_test.shape)
print('y_train:',y_train.shape)
print('y_test:',y_test.shape)

df.columns

"""#Logistic regression"""

from sklearn.linear_model import LogisticRegression

log = LogisticRegression()

log.fit(X_train,y_train)

print('train_accuracy:',log.score(X_train,y_train))
print('test_accuracy:',log.score(X_test,y_test))

y_pred = log.predict(X_test)
y_pred

from sklearn.metrics import confusion_matrix

print(confusion_matrix(y_test,y_pred))

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score

from sklearn.metrics import classification_report

print('accuracy_score:',accuracy_score(y_test,y_pred))
print('precision_score:',precision_score(y_test,y_pred))
print('recall_score:',recall_score(y_test,y_pred))
print('f1_score_score:',f1_score(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""#ROC-AUC curve"""

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import auc

pred_prob = log.predict_proba(X_test)

fpr,tpr,threshold = roc_curve(y_test,pred_prob[:,1])
print(auc(fpr,tpr))

plt.plot(fpr,tpr)
plt.plot([0,1],[0,1],'k--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC-AUC CURVE')
plt.plot()

"""## Random forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train,y_train)

print('training accuracy:',rf.score(X_train,y_train))
print('testing accuracy:',rf.score(X_test,y_test))

y_pred = rf.predict(X_test)

from sklearn.metrics import confusion_matrix , accuracy_score , precision_score , recall_score , f1_score

from sklearn.metrics import classification_report

print('accuracy:',accuracy_score(y_test,y_pred),'\n')
print('precision_score:',precision_score(y_test,y_pred),'\n')
print('recall_score:',recall_score(y_test,y_pred),'\n')
print('f1_score_score:',f1_score(y_test,y_pred),'\n')
print('classification report:','\n',classification_report(y_test,y_pred))

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=11)
knn.fit(X_train,y_train)

print('training accuracy:',knn.score(X_train,y_train))
print('testing accuracy:',knn.score(X_test,y_test))

y_pred = knn.predict(X_test)

print('accuracy:',accuracy_score(y_test,y_pred),'\n')
print('precision_score:',precision_score(y_test,y_pred),'\n')
print('recall_score:',recall_score(y_test,y_pred),'\n')
print('f1_score_score:',f1_score(y_test,y_pred),'\n')
print('classification report:','\n',classification_report(y_test,y_pred))

"""# XG Boosting"""

from xgboost import XGBClassifier
xg = XGBClassifier()
xg.fit(X_train,y_train)
print('training accuracy:',xg.score(X_train,y_train))
print('testing accuracy:',xg.score(X_test,y_test))

################################################################################################333##############################################

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

model_params={
    
'knn':{'model': KNeighborsClassifier(),'params':{'n_neighbors': [1,5,10,15,20,25] }},

'random_forest':{'model': RandomForestClassifier(),'params':{'n_estimators': [200,500],'max_features': ['auto', 'sqrt', 'log2'],
                                                             'max_depth' : [4,5,6,7,8],'criterion' :['gini', 'entropy'] }},
'logistic_regression':{'model':LogisticRegression(solver='saga', multi_class='auto'),'params':{'C':[0.001,.009,0.01,.09,1,5,10,25],'penalty': ['l1', 'l2'] }}
}

scores=[]
for model_name, mp in model_params.items():
  clf=GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)
  clf.fit(X, y)

  scores.append({
      'model':model_name,
      'best_score': clf.best_score_,
      'best_params': clf.best_params_
  })

scoreinfo=pd.DataFrame(scores, columns=['model','best_score','best_params'])
print(scoreinfo)
print(display(scoreinfo.to_string()))

